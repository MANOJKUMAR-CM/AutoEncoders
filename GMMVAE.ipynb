{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9832510-6a8c-4f30-89cc-aa6aae43ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa2cdb2-b6c1-4d70-8e53-1aa2b3c20748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2688a3c2-18c4-4cf1-a2a1-6b672e155146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "traindata = torchvision.datasets.MNIST('./', train=True, download=False, transform=torchvision.transforms.ToTensor())\n",
    "testdata = torchvision.datasets.MNIST('./', train=False, download=False, transform=torchvision.transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cd346c-d19a-4be1-b0a9-642d2759b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(traindata, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(testdata, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e110c28-5d71-49ac-b58a-67ee96af24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMVAE(nn.Module):\n",
    "    def __init__(self, input_dim=28*28, latent_dim=25, num_clusters=10):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_clusters = num_clusters\n",
    "\n",
    "        # Encoder\n",
    "        self.fc_enc = nn.Linear(input_dim, 250)\n",
    "        self.logits = nn.Linear(250, num_clusters)\n",
    "        self.means = nn.ModuleList([nn.Linear(250, latent_dim) for _ in range(num_clusters)])\n",
    "        self.logvars = nn.ModuleList([nn.Linear(250, latent_dim) for _ in range(num_clusters)])\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_dec = nn.Linear(latent_dim, 250)\n",
    "        self.output_layer = nn.Linear(250, input_dim)\n",
    "\n",
    "        # Priors\n",
    "        self.mean_priors = nn.Parameter(torch.randn(num_clusters, latent_dim))\n",
    "        self.logvar_priors = nn.Parameter(torch.zeros(num_clusters, latent_dim))  \n",
    "\n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc_dec(z))\n",
    "        return torch.sigmoid(self.output_layer(h))\n",
    "\n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc_enc(x))\n",
    "        logits = self.logits(h)\n",
    "        mean_list = [mean(h) for mean in self.means]\n",
    "        logvar_list = [logvar(h) for logvar in self.logvars]\n",
    "        q_c = F.softmax(logits, dim=-1)\n",
    "        return logits, q_c, mean_list, logvar_list\n",
    "\n",
    "    def gumbel_softmax(self, logits, tau, train=True):\n",
    "        return F.gumbel_softmax(logits, tau, hard=not train)\n",
    "\n",
    "    def kl_categorical(self, q_c):\n",
    "        # KL[q(c|x) || p(c)] where p(c) is uniform\n",
    "        log_q = torch.log(q_c + 1e-10)\n",
    "        log_uniform = math.log(1.0 / self.num_clusters)\n",
    "        kl = torch.sum(q_c * (log_q - log_uniform), dim=1)  # [batch]\n",
    "        return kl.mean()\n",
    "\n",
    "    def kl_gaussian(self, m_q, logvar_q, m_p, logvar_p):\n",
    "        return 0.5 * torch.sum(\n",
    "            (torch.exp(logvar_q) + (m_q - m_p).pow(2)) / torch.exp(logvar_p)\n",
    "            - 1 + logvar_p - logvar_q, dim=1\n",
    "        )  # shape [batch]\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        logits, q_c, mean_list, logvar_list = self.encoder(x)\n",
    "        c = self.gumbel_softmax(logits, tau=0.5, train=train)\n",
    "\n",
    "        # KL divergence losses\n",
    "        kl_c_loss = self.kl_categorical(q_c)\n",
    "        kl_z_loss = torch.zeros(batch_size, device=x.device)\n",
    "\n",
    "        z_samples = []\n",
    "        for i in range(self.num_clusters):\n",
    "            m_q = mean_list[i]\n",
    "            logvar_q = logvar_list[i]\n",
    "            std_q = torch.exp(0.5 * logvar_q)\n",
    "            eps = torch.randn_like(std_q)\n",
    "            z_i = m_q + std_q * eps\n",
    "            z_samples.append(z_i)\n",
    "\n",
    "            # KL(q || p)\n",
    "            m_p = self.mean_priors[i].unsqueeze(0).expand_as(m_q)\n",
    "            logvar_p = self.logvar_priors[i].unsqueeze(0).expand_as(logvar_q)\n",
    "            kl_i = self.kl_gaussian(m_q, logvar_q, m_p, logvar_p)\n",
    "            kl_z_loss += c[:, i] * kl_i\n",
    "\n",
    "        # Combine latent variables\n",
    "        z_stack = torch.stack(z_samples, dim=2)  # [batch, latent_dim, K]\n",
    "        z = torch.bmm(z_stack, c.unsqueeze(2)).squeeze(2)  # [batch, latent_dim]\n",
    "\n",
    "        # Decode\n",
    "        x_recon = self.decoder(z)\n",
    "\n",
    "        if x.shape!= x_recon.shape:\n",
    "            print(x_recon.shape)\n",
    "            print(x.shape)\n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / batch_size\n",
    "        kl_z_loss = kl_z_loss.mean()\n",
    "\n",
    "        total_loss = recon_loss + kl_c_loss + kl_z_loss\n",
    "        #print(total_loss)\n",
    "        return total_loss, x_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83212d58-ce0c-4819-9b04-5de184676859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd6e462-85e2-41eb-a083-93c93c336a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GMMVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a20893a-3ab0-44df-908e-2c3ccac04d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GMMVAE(\n",
       "  (fc_enc): Linear(in_features=784, out_features=250, bias=True)\n",
       "  (logits): Linear(in_features=250, out_features=10, bias=True)\n",
       "  (means): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=250, out_features=25, bias=True)\n",
       "  )\n",
       "  (logvars): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=250, out_features=25, bias=True)\n",
       "  )\n",
       "  (fc_dec): Linear(in_features=25, out_features=250, bias=True)\n",
       "  (output_layer): Linear(in_features=250, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dd6b146-64ae-43ed-8591-2cba6f47617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fb086ba-80b7-4073-b2b4-bacb37e47aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3189, Cosine Similarity: 0.8163\n",
      "Epoch 2, Loss: 2.0956, Cosine Similarity: 0.8531\n",
      "Epoch 3, Loss: 2.0849, Cosine Similarity: 0.8554\n",
      "Epoch 4, Loss: 2.0797, Cosine Similarity: 0.8567\n",
      "Epoch 5, Loss: 2.0823, Cosine Similarity: 0.8565\n",
      "Epoch 6, Loss: 2.0799, Cosine Similarity: 0.8570\n",
      "Epoch 7, Loss: 2.0812, Cosine Similarity: 0.8571\n",
      "Epoch 8, Loss: 2.0892, Cosine Similarity: 0.8562\n",
      "Epoch 9, Loss: 2.0906, Cosine Similarity: 0.8559\n",
      "Epoch 10, Loss: 2.0933, Cosine Similarity: 0.8556\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_loss =0.0\n",
    "    cosine_sims = []\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        loss, x_recon = model(data, train=True)\n",
    "        \n",
    "        flat_data = data.view(-1, 28*28)\n",
    "        #recons_loss = F.binary_cross_entropy_with_logits(flat_data, x_recon)\n",
    "        #loss = -recons_loss + KL_total\n",
    "        #print(recons_loss)\n",
    "        #print(KL_total)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        \n",
    "        cos_sim = F.cosine_similarity(flat_data, x_recon, dim=1)  \n",
    "        cosine_sims.append(cos_sim)\n",
    "        \n",
    "        # Aggregate cosine similarity across all batches\n",
    "    epoch_cosine_similarity = torch.cat(cosine_sims).mean().item()\n",
    "\n",
    "    print(f'Epoch {i + 1}, Loss: {train_loss / len(train_loader.dataset):.4f}, Cosine Similarity: {epoch_cosine_similarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a4cf2e7-82cc-46c2-9364-802aa0cead37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2108, Cosine Similarity: 0.8389\n",
      "Epoch 2, Loss: 2.0364, Cosine Similarity: 0.8683\n",
      "Epoch 3, Loss: 2.0365, Cosine Similarity: 0.8692\n",
      "Epoch 4, Loss: 2.0352, Cosine Similarity: 0.8698\n",
      "Epoch 5, Loss: 2.0371, Cosine Similarity: 0.8698\n",
      "Epoch 6, Loss: 2.0442, Cosine Similarity: 0.8685\n",
      "Epoch 7, Loss: 2.0398, Cosine Similarity: 0.8693\n",
      "Epoch 8, Loss: 2.0446, Cosine Similarity: 0.8694\n",
      "Epoch 9, Loss: 2.0394, Cosine Similarity: 0.8700\n",
      "Epoch 10, Loss: 2.0474, Cosine Similarity: 0.8692\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_loss =0.0\n",
    "    cosine_sims = []\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        loss, x_recon = model(data, train=True)\n",
    "        \n",
    "        flat_data = data.view(-1, 28*28)\n",
    "        #recons_loss = F.binary_cross_entropy_with_logits(flat_data, x_recon)\n",
    "        #loss = -recons_loss + KL_total\n",
    "        #print(recons_loss)\n",
    "        #print(KL_total)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        \n",
    "        cos_sim = F.cosine_similarity(flat_data, x_recon, dim=1)  \n",
    "        cosine_sims.append(cos_sim)\n",
    "        \n",
    "        # Aggregate cosine similarity across all batches\n",
    "    epoch_cosine_similarity = torch.cat(cosine_sims).mean().item()\n",
    "\n",
    "    print(f'Epoch {i + 1}, Loss: {train_loss / len(train_loader.dataset):.4f}, Cosine Similarity: {epoch_cosine_similarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c37c1f9-a285-4cda-96d1-320a0255db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Cosine Similarity: 0.8769\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_cosine = []\n",
    "test_loss = 0.0\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "\n",
    "    loss, x_recon = model(data, train=False)\n",
    "    flat_data = data.view(-1, 28*28)\n",
    "    test_loss+=loss.item()\n",
    "\n",
    "    cos_sim = F.cosine_similarity(flat_data, x_recon, dim=1)  \n",
    "    test_cosine.append(cos_sim)\n",
    "\n",
    "cosine_similarity = torch.cat(test_cosine).mean().item()\n",
    "print(f\"Test_Cosine Similarity: {round(cosine_similarity, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade28f5-0ccf-491c-bf45-0fe851f0cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
